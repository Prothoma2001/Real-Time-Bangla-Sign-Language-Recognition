{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb6NbEiYWaAilUR6VZVREL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition/blob/main/Bangla_Sign_Language_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktu90t1d4--g",
        "outputId": "eb3d9851-43e3-47d8-966e-67f2632f241b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset 2"
      ],
      "metadata": {
        "id": "FnqdZot2v_nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwGdndlv-W1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee9538e-2eb2-41c6-d6d5-a179c0b8f5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "739/739 [==============================] - 21s 11ms/step - loss: 2.6580 - accuracy: 0.2889 - val_loss: 1.2477 - val_accuracy: 0.6212\n",
            "Epoch 2/10\n",
            "739/739 [==============================] - 8s 10ms/step - loss: 0.9949 - accuracy: 0.6991 - val_loss: 0.7823 - val_accuracy: 0.7559\n",
            "Epoch 3/10\n",
            "739/739 [==============================] - 7s 10ms/step - loss: 0.6139 - accuracy: 0.8014 - val_loss: 0.5820 - val_accuracy: 0.8191\n",
            "Epoch 4/10\n",
            "739/739 [==============================] - 8s 11ms/step - loss: 0.4317 - accuracy: 0.8633 - val_loss: 0.4142 - val_accuracy: 0.8665\n",
            "Epoch 5/10\n",
            "739/739 [==============================] - 8s 11ms/step - loss: 0.3129 - accuracy: 0.8989 - val_loss: 0.3823 - val_accuracy: 0.8898\n",
            "Epoch 6/10\n",
            "739/739 [==============================] - 8s 10ms/step - loss: 0.2414 - accuracy: 0.9258 - val_loss: 0.3448 - val_accuracy: 0.9103\n",
            "Epoch 7/10\n",
            "739/739 [==============================] - 7s 10ms/step - loss: 0.1838 - accuracy: 0.9409 - val_loss: 0.3617 - val_accuracy: 0.9076\n",
            "Epoch 8/10\n",
            "739/739 [==============================] - 8s 11ms/step - loss: 0.1414 - accuracy: 0.9560 - val_loss: 0.3317 - val_accuracy: 0.9151\n",
            "Epoch 9/10\n",
            "739/739 [==============================] - 7s 10ms/step - loss: 0.1302 - accuracy: 0.9605 - val_loss: 0.3126 - val_accuracy: 0.9234\n",
            "Epoch 10/10\n",
            "739/739 [==============================] - 8s 11ms/step - loss: 0.1173 - accuracy: 0.9665 - val_loss: 0.3067 - val_accuracy: 0.9234\n",
            "317/317 [==============================] - 1s 4ms/step - loss: 0.3067 - accuracy: 0.9234\n",
            "Test loss: 0.30671560764312744\n",
            "Test accuracy: 0.9233807325363159\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "\n",
        "# Define the path to your dataset directory\n",
        "dataset_path = \"/content/drive/MyDrive/BdSL_OPSA22_STATIC2\"\n",
        "\n",
        "# Define the dimensions of your images\n",
        "img_width, img_height = 64,64\n",
        "\n",
        "# Define the batch size and number of epochs for training\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "\n",
        "# Initialize lists to hold the data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop over the dataset directory\n",
        "for i, class_folder in enumerate(os.listdir(dataset_path)):\n",
        "    class_folder_path = os.path.join(dataset_path, class_folder)\n",
        "    # Loop over the images in each class folder\n",
        "    for image_name in os.listdir(class_folder_path):\n",
        "        image_path = os.path.join(class_folder_path, image_name)\n",
        "        # Load the image, resize to the desired dimensions, and convert to gray scale\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (img_width, img_height))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # fix\n",
        "        # Add the pre-processed image and its label to the lists\n",
        "        data.append(image)\n",
        "        labels.append(i)\n",
        "\n",
        "# Convert the data and labels lists to numpy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "train_data = train_data.astype(\"float32\") / 255.0\n",
        "test_data = test_data.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(labels))\n",
        "train_labels = np.eye(num_classes)[train_labels]\n",
        "test_labels = np.eye(num_classes)[test_labels]\n",
        "\n",
        "#creating model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=train_data.shape[1:]))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_data, test_labels))\n",
        "\n",
        "model.save(\"OurMainModel_2.h5\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, batch_size=batch_size)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Get the predicted class probabilities for the test set\n",
        "y_pred = model.predict(test_data)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert the one-hot encoded test labels to class labels\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Compute the classification report\n",
        "class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Confusion matrix:\\n\", conf_matrix)\n",
        "print(\"Classification report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "YEvk1fuB-bT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ce8426-d4a1-419b-9338-95505d06c80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 1s 4ms/step\n",
            "Confusion matrix:\n",
            " [[50  0  0 ...  0  0  0]\n",
            " [ 0 28  1 ...  0  0  0]\n",
            " [ 0  1 34 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ... 46  0  1]\n",
            " [ 0  0  0 ...  0 29  0]\n",
            " [ 0  0  0 ...  1  0 69]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91        54\n",
            "           1       0.72      0.82      0.77        34\n",
            "           2       0.97      0.87      0.92        39\n",
            "           3       0.78      0.88      0.83        73\n",
            "           4       0.95      0.95      0.95        59\n",
            "           5       0.93      1.00      0.97        43\n",
            "           6       0.84      0.97      0.90        37\n",
            "           7       0.96      0.83      0.89        53\n",
            "           8       0.78      0.90      0.84        20\n",
            "           9       0.98      0.89      0.93        61\n",
            "          10       0.96      0.96      0.96        55\n",
            "          11       0.95      0.93      0.94        57\n",
            "          12       0.87      0.90      0.88        50\n",
            "          13       1.00      0.94      0.97        49\n",
            "          14       0.91      0.69      0.78        29\n",
            "          15       0.93      0.93      0.93        40\n",
            "          16       0.96      0.96      0.96        57\n",
            "          17       0.82      0.87      0.84        53\n",
            "          18       1.00      0.86      0.92        57\n",
            "          19       0.87      0.92      0.89        74\n",
            "          20       0.96      1.00      0.98        45\n",
            "          21       0.90      0.90      0.90        63\n",
            "          22       0.96      0.88      0.92        50\n",
            "          23       0.95      0.91      0.93        65\n",
            "          24       0.98      0.96      0.97        47\n",
            "          25       0.91      0.94      0.92        77\n",
            "          26       0.98      0.96      0.97        54\n",
            "          27       0.89      0.94      0.92        36\n",
            "          28       0.96      0.95      0.96        57\n",
            "          29       0.97      0.93      0.95        69\n",
            "          30       0.97      1.00      0.98        61\n",
            "          31       0.92      0.89      0.90        37\n",
            "          32       0.89      0.88      0.88        73\n",
            "          33       0.95      0.98      0.97        60\n",
            "          34       0.97      0.92      0.94        72\n",
            "          35       0.95      0.96      0.96        80\n",
            "          36       0.69      0.98      0.81        52\n",
            "          37       0.94      0.94      0.94        48\n",
            "          38       0.98      0.83      0.90        48\n",
            "          39       0.99      0.91      0.95        91\n",
            "          40       0.97      0.98      0.97        99\n",
            "          41       1.00      0.95      0.97        74\n",
            "          42       0.85      0.82      0.84        28\n",
            "          43       0.90      0.92      0.91        50\n",
            "          44       0.91      1.00      0.95        29\n",
            "          45       0.96      0.95      0.95        73\n",
            "\n",
            "    accuracy                           0.92      2532\n",
            "   macro avg       0.92      0.92      0.92      2532\n",
            "weighted avg       0.93      0.92      0.92      2532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels.argmax(axis=1), y_pred)\n",
        "\n",
        "# Compute the sensitivity and specificity\n",
        "sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)"
      ],
      "metadata": {
        "id": "RDrjj3Gs-eMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdaeab4c-cfc7-4f83-d532-a15bca4c283b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity: 1.0\n",
            "Specificity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_data)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(test_labels, predictions)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = confusion[i, i]\n",
        "    fp = np.sum(confusion[:, i]) - tp\n",
        "    fn = np.sum(confusion[i, :]) - tp\n",
        "    tn = np.sum(confusion) - tp - fp - fn\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(\"Class {}: Sensitivity = {:.2f}, Specificity = {:.2f}, Accuracy = {:.2f}\".format(i, sensitivity, specificity, accuracy)) "
      ],
      "metadata": {
        "id": "Ln5MzJ8j-jpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccb26a1-8eb7-48d0-f1cf-e392ed3ff07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 1s 6ms/step\n",
            "Class 0: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 1: Sensitivity = 0.82, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 2: Sensitivity = 0.87, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 3: Sensitivity = 0.88, Specificity = 0.99, Accuracy = 0.99\n",
            "Class 4: Sensitivity = 0.95, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 5: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 6: Sensitivity = 0.97, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 7: Sensitivity = 0.83, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 8: Sensitivity = 0.90, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 9: Sensitivity = 0.89, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 10: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 11: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 12: Sensitivity = 0.90, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 13: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 14: Sensitivity = 0.69, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 15: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 16: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 17: Sensitivity = 0.87, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 18: Sensitivity = 0.86, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 19: Sensitivity = 0.92, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 20: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 21: Sensitivity = 0.90, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 22: Sensitivity = 0.88, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 23: Sensitivity = 0.91, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 24: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 25: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 26: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 27: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 28: Sensitivity = 0.95, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 29: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 30: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 31: Sensitivity = 0.89, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 32: Sensitivity = 0.88, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 33: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 34: Sensitivity = 0.92, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 35: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 36: Sensitivity = 0.98, Specificity = 0.99, Accuracy = 0.99\n",
            "Class 37: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 38: Sensitivity = 0.83, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 39: Sensitivity = 0.91, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 40: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 41: Sensitivity = 0.95, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 42: Sensitivity = 0.82, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 43: Sensitivity = 0.92, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 44: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 45: Sensitivity = 0.95, Specificity = 1.00, Accuracy = 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3HSW3rA8ODY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset 1"
      ],
      "metadata": {
        "id": "iet3RWkPGVW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db20f53d-858b-4691-b50e-503d5fa9d282",
        "id": "qVwQbOseGV6E"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08edecc2-fa95-4e44-8912-01e358d936d1",
        "id": "-cAKyn6VGV6G"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2135/2135 [==============================] - 35s 11ms/step - loss: 2.0438 - accuracy: 0.4456 - val_loss: 0.3764 - val_accuracy: 0.8841\n",
            "Epoch 2/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.3924 - accuracy: 0.8769 - val_loss: 0.1965 - val_accuracy: 0.9351\n",
            "Epoch 3/10\n",
            "2135/2135 [==============================] - 24s 11ms/step - loss: 0.2427 - accuracy: 0.9211 - val_loss: 0.1578 - val_accuracy: 0.9481\n",
            "Epoch 4/10\n",
            "2135/2135 [==============================] - 24s 11ms/step - loss: 0.1949 - accuracy: 0.9355 - val_loss: 0.1358 - val_accuracy: 0.9568\n",
            "Epoch 5/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.1558 - accuracy: 0.9472 - val_loss: 0.0926 - val_accuracy: 0.9749\n",
            "Epoch 6/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.1386 - accuracy: 0.9530 - val_loss: 0.1085 - val_accuracy: 0.9675\n",
            "Epoch 7/10\n",
            "2135/2135 [==============================] - 24s 11ms/step - loss: 0.1210 - accuracy: 0.9606 - val_loss: 0.0912 - val_accuracy: 0.9729\n",
            "Epoch 8/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.1118 - accuracy: 0.9637 - val_loss: 0.0876 - val_accuracy: 0.9683\n",
            "Epoch 9/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.0977 - accuracy: 0.9673 - val_loss: 0.0807 - val_accuracy: 0.9728\n",
            "Epoch 10/10\n",
            "2135/2135 [==============================] - 21s 10ms/step - loss: 0.0997 - accuracy: 0.9671 - val_loss: 0.0745 - val_accuracy: 0.9772\n",
            "915/915 [==============================] - 3s 4ms/step - loss: 0.0745 - accuracy: 0.9772\n",
            "Test loss: 0.0744999423623085\n",
            "Test accuracy: 0.9771795868873596\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "\n",
        "# Define the path to your dataset directory\n",
        "dataset_path = \"/content/drive/MyDrive/BdSL_OPSA22_STATIC1\"\n",
        "\n",
        "# Define the dimensions of your images\n",
        "img_width, img_height = 64,64\n",
        "\n",
        "# Define the batch size and number of epochs for training\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "\n",
        "# Initialize lists to hold the data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop over the dataset directory\n",
        "for i, class_folder in enumerate(os.listdir(dataset_path)):\n",
        "    class_folder_path = os.path.join(dataset_path, class_folder)\n",
        "    # Loop over the images in each class folder\n",
        "    for image_name in os.listdir(class_folder_path):\n",
        "        image_path = os.path.join(class_folder_path, image_name)\n",
        "        # Load the image, resize to the desired dimensions, and convert to gray scale\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (img_width, img_height))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # fix\n",
        "        # Add the pre-processed image and its label to the lists\n",
        "        data.append(image)\n",
        "        labels.append(i)\n",
        "\n",
        "# Convert the data and labels lists to numpy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "train_data = train_data.astype(\"float32\") / 255.0\n",
        "test_data = test_data.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = len(np.unique(labels))\n",
        "train_labels = np.eye(num_classes)[train_labels]\n",
        "test_labels = np.eye(num_classes)[test_labels]\n",
        "\n",
        "#creating model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=train_data.shape[1:]))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_data, test_labels))\n",
        "\n",
        "model.save(\"OurMainModel_2.h5\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, batch_size=batch_size)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Get the predicted class probabilities for the test set\n",
        "y_pred = model.predict(test_data)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert the one-hot encoded test labels to class labels\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Compute the classification report\n",
        "class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Confusion matrix:\\n\", conf_matrix)\n",
        "print(\"Classification report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bb5a4c-dcac-469a-c99c-71275fd6af2e",
        "id": "eo20hZ8vGV6H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229/229 [==============================] - 1s 4ms/step\n",
            "Confusion matrix:\n",
            " [[140   0   0 ...   0   0   0]\n",
            " [  0  62   0 ...   0   0   0]\n",
            " [  0   0 101 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 129   0   0]\n",
            " [  0   0   0 ...   0 164   0]\n",
            " [  0   0   0 ...   0   0 126]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       142\n",
            "           1       0.97      0.78      0.87        79\n",
            "           2       1.00      0.99      1.00       102\n",
            "           3       0.95      0.96      0.95        73\n",
            "           4       0.90      0.99      0.94       109\n",
            "           5       0.94      0.83      0.88        41\n",
            "           6       0.98      0.97      0.98       156\n",
            "           7       0.94      0.94      0.94        63\n",
            "           8       1.00      1.00      1.00       121\n",
            "           9       0.98      0.99      0.99       123\n",
            "          10       0.81      0.85      0.83        87\n",
            "          11       0.98      0.93      0.95       169\n",
            "          12       0.89      0.91      0.90       128\n",
            "          13       0.99      0.98      0.99       148\n",
            "          14       0.66      0.99      0.79        70\n",
            "          15       1.00      1.00      1.00       155\n",
            "          16       0.99      1.00      1.00       163\n",
            "          17       0.87      0.61      0.72        77\n",
            "          18       1.00      0.99      0.99       214\n",
            "          19       0.99      0.99      0.99       174\n",
            "          20       1.00      0.99      0.99       219\n",
            "          21       1.00      0.99      0.99       218\n",
            "          22       1.00      1.00      1.00       222\n",
            "          23       1.00      1.00      1.00       195\n",
            "          24       0.99      1.00      0.99       247\n",
            "          25       1.00      1.00      1.00       240\n",
            "          26       1.00      0.99      0.99       215\n",
            "          27       1.00      1.00      1.00       377\n",
            "          28       0.97      0.99      0.98       389\n",
            "          29       0.91      0.94      0.93        80\n",
            "          30       0.94      0.98      0.96        93\n",
            "          31       1.00      1.00      1.00       289\n",
            "          32       1.00      0.97      0.99       313\n",
            "          33       1.00      0.99      0.99        69\n",
            "          34       1.00      0.98      0.99       156\n",
            "          35       0.99      0.99      0.99       141\n",
            "          36       1.00      0.99      1.00       137\n",
            "          37       0.98      0.98      0.98       113\n",
            "          38       0.99      0.99      0.99       144\n",
            "          39       0.99      1.00      1.00       167\n",
            "          40       0.92      0.93      0.92       176\n",
            "          41       1.00      0.99      1.00       162\n",
            "          42       0.99      1.00      0.99       140\n",
            "          43       0.99      1.00      1.00       129\n",
            "          44       0.99      0.99      0.99       165\n",
            "          45       1.00      0.98      0.99       128\n",
            "\n",
            "    accuracy                           0.98      7318\n",
            "   macro avg       0.97      0.96      0.96      7318\n",
            "weighted avg       0.98      0.98      0.98      7318\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels.argmax(axis=1), y_pred)\n",
        "\n",
        "# Compute the sensitivity and specificity\n",
        "sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0edb96-8e5e-4491-c7c9-23e9e8b2fbe3",
        "id": "t3NnT0X7GV6I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity: 1.0\n",
            "Specificity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_data)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(test_labels, predictions)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = confusion[i, i]\n",
        "    fp = np.sum(confusion[:, i]) - tp\n",
        "    fn = np.sum(confusion[i, :]) - tp\n",
        "    tn = np.sum(confusion) - tp - fp - fn\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(\"Class {}: Sensitivity = {:.2f}, Specificity = {:.2f}, Accuracy = {:.2f}\".format(i, sensitivity, specificity, accuracy)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a933ec4-a56e-45e3-a0ae-125cf073f48d",
        "id": "Pbut-ho9GV6J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229/229 [==============================] - 1s 4ms/step\n",
            "Class 0: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 1: Sensitivity = 0.78, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 2: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 3: Sensitivity = 0.96, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 4: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 5: Sensitivity = 0.83, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 6: Sensitivity = 0.97, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 7: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 8: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 9: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 10: Sensitivity = 0.85, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 11: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 12: Sensitivity = 0.91, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 13: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 14: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 15: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 16: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 17: Sensitivity = 0.61, Specificity = 1.00, Accuracy = 0.99\n",
            "Class 18: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 19: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 20: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 21: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 22: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 23: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 24: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 25: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 26: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 27: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 28: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 29: Sensitivity = 0.94, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 30: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 31: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 32: Sensitivity = 0.97, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 33: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 34: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 35: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 36: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 37: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 38: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 39: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 40: Sensitivity = 0.93, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 41: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 42: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 43: Sensitivity = 1.00, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 44: Sensitivity = 0.99, Specificity = 1.00, Accuracy = 1.00\n",
            "Class 45: Sensitivity = 0.98, Specificity = 1.00, Accuracy = 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check model summary\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "\n",
        "#creating model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(64,64,3)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(46, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U3E6EuCsQ_R",
        "outputId": "1240a3a7-f4f9-4c90-b421-f5af11dca264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 32)        896       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 32, 32, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 16384)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              16778240  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 46)                47150     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,940,558\n",
            "Trainable params: 17,940,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}
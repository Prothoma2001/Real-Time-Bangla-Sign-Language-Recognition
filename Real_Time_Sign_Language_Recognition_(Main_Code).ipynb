{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMISyoTaSHjy3p9CqPNLW2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "cfa43dca-e786-4d24-a08c-a9e5d095f97c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "eb033590-5438-43eb-ef1b-f52d6fc01396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "78f89888-19c4-46be-c6ab-073b9ef9e130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "d991e240-bc7e-4a03-b3a1-cc83a0e4521e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1582  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1583  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1584  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1585  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1586  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "3afd24c5-90ae-4e79-8d0b-a42f070731ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "534  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "535  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "536  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "537  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "538  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "f49d321f-b61e-4885-9100-4686748e886a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1587\n",
            "Total videos for testing: 539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "633          633  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "1388        1388  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   26\n",
              "274          274  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   86\n",
              "993          993  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   39\n",
              "1129        1129  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    3\n",
              "883          883  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "814          814  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   52\n",
              "231          231  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   82\n",
              "691          691  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   55\n",
              "712          712  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>1388</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>814</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>231</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>691</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>712</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\"))\n",
        "    model.add(Dense(units=6144,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "d6464f66-cb64-4a5a-a532-10c3e624da63"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "2676ebfd-acae-4e01-8f11-acbbba18625f"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "a5cd84c2-57d8-451d-9bbf-9565964923fb"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9951 - accuracy: 0.6647\n",
            "Epoch 1: val_loss improved from inf to 1.00104, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 493ms/step - loss: 1.9951 - accuracy: 0.6647 - val_loss: 1.0010 - val_accuracy: 0.4444\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9504\n",
            "Epoch 2: val_loss improved from 1.00104 to 0.53204, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 474ms/step - loss: 0.1384 - accuracy: 0.9504 - val_loss: 0.5320 - val_accuracy: 0.8444\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9980\n",
            "Epoch 3: val_loss did not improve from 0.53204\n",
            "16/16 [==============================] - 7s 454ms/step - loss: 0.0142 - accuracy: 0.9980 - val_loss: 1.7732 - val_accuracy: 0.4778\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980\n",
            "Epoch 4: val_loss improved from 0.53204 to 0.39095, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 466ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.3909 - val_accuracy: 0.9000\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9980\n",
            "Epoch 5: val_loss did not improve from 0.39095\n",
            "16/16 [==============================] - 7s 452ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.7858 - val_accuracy: 0.8222\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 7.9930e-04 - accuracy: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.39095\n",
            "16/16 [==============================] - 7s 452ms/step - loss: 7.9930e-04 - accuracy: 1.0000 - val_loss: 1.3670 - val_accuracy: 0.6556\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 5.9319e-04 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.39095\n",
            "16/16 [==============================] - 7s 451ms/step - loss: 5.9319e-04 - accuracy: 1.0000 - val_loss: 1.0992 - val_accuracy: 0.7556\n",
            "7/7 [==============================] - 1s 138ms/step - loss: 0.2972 - accuracy: 0.9330\n",
            "Test accuracy: 93.3%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
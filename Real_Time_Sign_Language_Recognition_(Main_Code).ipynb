{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnQuSUYlRCv40R5Tx7a8YZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "387703e7-5141-4427-ff16-0df43e971951"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "a71212fe-61b5-4cbc-d6e3-68c70e59e95d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "17429f5f-0c80-4641-b63d-12186b017066"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "05589c8f-a995-4da8-bab3-6b37207ebf9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "2087  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2088  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2089  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2090  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2091  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "25c8263f-f1f7-44ab-f07e-47b70de984ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "737  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "738  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "739  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "740  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "741  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "55d05849-36dc-45f0-f545-bc0d1032ee35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 2092\n",
            "Total videos for testing: 742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "43            43  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   96\n",
              "37            37  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   91\n",
              "960          960  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   54\n",
              "245          245  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   84\n",
              "2058        2058  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    0\n",
              "1621        1621  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   28\n",
              "1825        1825  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   22\n",
              "1548        1548  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   29\n",
              "375          375  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   87\n",
              "2004        2004  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   10"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0566df1-7155-4478-bdd4-106ed7e49fab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>960</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>245</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2058</th>\n",
              "      <td>2058</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>1621</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1825</th>\n",
              "      <td>1825</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>1548</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>375</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>2004</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0566df1-7155-4478-bdd4-106ed7e49fab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0566df1-7155-4478-bdd4-106ed7e49fab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0566df1-7155-4478-bdd4-106ed7e49fab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\")) #base_filter=64\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) \n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=128\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=256\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "e70dac78-52fa-4997-9375-12282229d544"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "3e606ee0-6ff1-4746-f04c-25b48a5f60c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)  #base_dropout=0.5\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() \n",
        "\n",
        "run_time1 = (time.time()- start_time)\n",
        "print(\"-----%s seconds-----\" % run_time1)"
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "5b56a524-e46c-4248-ef61-2b619118cc6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.9871 - accuracy: 0.5655\n",
            "Epoch 1: val_loss improved from inf to 4.59282, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 2s 50ms/step - loss: 2.9871 - accuracy: 0.5655 - val_loss: 4.5928 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/7\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2395 - accuracy: 0.9087\n",
            "Epoch 2: val_loss improved from 4.59282 to 1.35646, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.2205 - accuracy: 0.9147 - val_loss: 1.3565 - val_accuracy: 0.3556\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9861\n",
            "Epoch 3: val_loss improved from 1.35646 to 0.64124, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0457 - accuracy: 0.9861 - val_loss: 0.6412 - val_accuracy: 0.7667\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9980\n",
            "Epoch 4: val_loss did not improve from 0.64124\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 1.4379 - val_accuracy: 0.5333\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 5: val_loss did not improve from 0.64124\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.4921 - val_accuracy: 0.5778\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 0.64124\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.7727 - val_accuracy: 0.7778\n",
            "Epoch 7/7\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.64124\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.4243 - val_accuracy: 0.6667\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.3754 - accuracy: 0.9018\n",
            "Test accuracy: 90.18%\n",
            "-----347.35802936553955 seconds-----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4oexck_nj-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
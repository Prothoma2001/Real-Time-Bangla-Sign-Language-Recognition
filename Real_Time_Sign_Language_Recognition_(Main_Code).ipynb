{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPffLaPi1/16QHK/A6iG1Ub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "f45862dd-dc70-487d-cb91-fd9dc1047856"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "09c8292e-611f-4a3a-8b52-1fbfde37018e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "1570acff-cf0d-432b-ff69-e62f4cc14b39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "04b456b6-c117-44cd-f7ee-bb7d5068350c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1482  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1483  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1484  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1485  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1486  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "71925490-98d4-435e-97a1-11a4ff387171"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  96  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "434  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "435  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "436  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "437  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "438  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "08582a7e-fab7-48f0-ba51-d2682b5ef530"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1487\n",
            "Total videos for testing: 439\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "495          495  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   68\n",
              "122          122  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   93\n",
              "627          627  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    6\n",
              "870          870  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   53\n",
              "401          401  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    8\n",
              "1123        1123  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   32\n",
              "594          594  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "334          334  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   80\n",
              "180          180  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   88\n",
              "1409        1409  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   10"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47068ccc-a1da-424f-9ded-e9f06c426802\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>495</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>122</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>627</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>870</th>\n",
              "      <td>870</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>401</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1123</th>\n",
              "      <td>1123</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>594</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>334</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>180</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>1409</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47068ccc-a1da-424f-9ded-e9f06c426802')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47068ccc-a1da-424f-9ded-e9f06c426802 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47068ccc-a1da-424f-9ded-e9f06c426802');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor():\n",
        "    __all__ = ['Inception3', 'inception_v3', 'InceptionOutputs', '_InceptionOutputs']\n",
        "\n",
        "\n",
        "    model_urls = {\n",
        "    # Inception v3 ported from TensorFlow\n",
        "       'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
        "    }\n",
        "\n",
        "    InceptionOutputs = namedtuple('InceptionOutputs', ['logits', 'aux_logits'])\n",
        "    InceptionOutputs.__annotations__ = {'logits': torch.Tensor, 'aux_logits': Optional[torch.Tensor]}\n",
        "\n",
        "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
        "# _InceptionOutputs set here for backwards compat\n",
        "    _InceptionOutputs = InceptionOutputs\n",
        "\n",
        "\n",
        "    def inception_v3(pretrained=False, progress=True, **kwargs):\n",
        "      r\"\"\"Inception v3 model architecture from\n",
        "      `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
        "      .. note::\n",
        "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
        "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
        "      Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n",
        "            Default: *True*\n",
        "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
        "            was trained on ImageNet. Default: *False*\n",
        "      \"\"\"\n",
        "      if pretrained:\n",
        "        if 'transform_input' not in kwargs:\n",
        "            kwargs['transform_input'] = True\n",
        "        if 'aux_logits' in kwargs:\n",
        "            original_aux_logits = kwargs['aux_logits']\n",
        "            kwargs['aux_logits'] = True\n",
        "        else:\n",
        "            original_aux_logits = True\n",
        "        kwargs['init_weights'] = False  # we are loading weights from a pretrained model\n",
        "        model = Inception3(**kwargs)\n",
        "        state_dict = load_state_dict_from_url(model_urls['inception_v3_google'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "        if not original_aux_logits:\n",
        "            model.aux_logits = False\n",
        "            del model.AuxLogits\n",
        "        return model\n",
        "\n",
        "      return Inception3(**kwargs)\n",
        "\n",
        "\n",
        "    class Inception3(nn.Module):\n",
        "\n",
        "      def __init__(self, num_classes=1000, aux_logits=True, transform_input=False,\n",
        "                 inception_blocks=None, init_weights=None):\n",
        "        super(Inception3, self).__init__()\n",
        "        if inception_blocks is None:\n",
        "            inception_blocks = [\n",
        "                BasicConv2d, InceptionA, InceptionB, InceptionC,\n",
        "                InceptionD, InceptionE, InceptionAux\n",
        "            ]\n",
        "        if init_weights is None:\n",
        "            warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n",
        "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
        "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
        "            init_weights = True\n",
        "        assert len(inception_blocks) == 7\n",
        "        conv_block = inception_blocks[0]\n",
        "        inception_a = inception_blocks[1]\n",
        "        inception_b = inception_blocks[2]\n",
        "        inception_c = inception_blocks[3]\n",
        "        inception_d = inception_blocks[4]\n",
        "        inception_e = inception_blocks[5]\n",
        "        inception_aux = inception_blocks[6]\n",
        "\n",
        "        self.aux_logits = aux_logits\n",
        "        self.transform_input = transform_input\n",
        "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
        "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
        "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
        "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
        "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
        "        self.Mixed_6a = inception_b(288)\n",
        "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
        "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
        "        if aux_logits:\n",
        "            self.AuxLogits = inception_aux(768, num_classes)\n",
        "        self.Mixed_7a = inception_d(768)\n",
        "        self.Mixed_7b = inception_e(1280)\n",
        "        self.Mixed_7c = inception_e(2048)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "        if init_weights:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                    import scipy.stats as stats\n",
        "                    stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
        "                    X = stats.truncnorm(-2, 2, scale=stddev)\n",
        "                    values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
        "                    values = values.view(m.weight.size())\n",
        "                    with torch.no_grad():\n",
        "                        m.weight.copy_(values)\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "      def _transform_input(self, x):\n",
        "        if self.transform_input:\n",
        "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "        return x\n",
        "\n",
        "      def _forward(self, x):\n",
        "        # N x 3 x 299 x 299\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # N x 32 x 149 x 149\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # N x 32 x 147 x 147\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # N x 64 x 147 x 147\n",
        "        x = self.maxpool1(x)\n",
        "        # N x 64 x 73 x 73\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # N x 80 x 73 x 73\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # N x 192 x 71 x 71\n",
        "        x = self.maxpool2(x)\n",
        "        # N x 192 x 35 x 35\n",
        "        x = self.Mixed_5b(x)\n",
        "        # N x 256 x 35 x 35\n",
        "        x = self.Mixed_5c(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_5d(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_6a(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6b(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6c(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6d(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6e(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if aux_defined:\n",
        "            aux = self.AuxLogits(x)\n",
        "        else:\n",
        "            aux = None\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_7a(x)\n",
        "        # N x 1280 x 8 x 8\n",
        "        x = self.Mixed_7b(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        x = self.Mixed_7c(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        # Adaptive average pooling\n",
        "        x = self.avgpool(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = self.dropout(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        # N x 2048\n",
        "        x = self.fc(x)\n",
        "        # N x 1000 (num_classes)\n",
        "        return x, aux\n",
        "\n",
        "      @torch.jit.unused\n",
        "      def eager_outputs(self, x, aux):\n",
        "        # type: (Tensor, Optional[Tensor]) -> InceptionOutputs\n",
        "        if self.training and self.aux_logits:\n",
        "            return InceptionOutputs(x, aux)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self._transform_input(x)\n",
        "        x, aux = self._forward(x)\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if torch.jit.is_scripting():\n",
        "            if not aux_defined:\n",
        "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
        "            return InceptionOutputs(x, aux)\n",
        "        else:\n",
        "            return self.eager_outputs(x, aux)\n",
        "\n",
        "\n",
        "    class InceptionA(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, pool_features, conv_block=None):\n",
        "        super(InceptionA, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
        "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
        "\n",
        "      def _forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "      def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "    class InceptionB(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, conv_block=None):\n",
        "        super(InceptionB, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
        "\n",
        "      def _forward(self, x):\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "\n",
        "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "      def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "    class InceptionC(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, channels_7x7, conv_block=None):\n",
        "        super(InceptionC, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "\n",
        "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "      def _forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "      def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "    class InceptionD(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, conv_block=None):\n",
        "        super(InceptionD, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
        "\n",
        "      def _forward(self, x):\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "        branch7x7x3 = self.branch7x7x3_1(x)\n",
        "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "      def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "    class InceptionE(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, conv_block=None):\n",
        "        super(InceptionE, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "      def _forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "      def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "    class InceptionAux(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, num_classes, conv_block=None):\n",
        "        super(InceptionAux, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
        "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
        "        self.conv1.stddev = 0.01\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "        self.fc.stddev = 0.001\n",
        "\n",
        "      def forward(self, x):\n",
        "        # N x 768 x 17 x 17\n",
        "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
        "        # N x 768 x 5 x 5\n",
        "        x = self.conv0(x)\n",
        "        # N x 128 x 5 x 5\n",
        "        x = self.conv1(x)\n",
        "        # N x 768 x 1 x 1\n",
        "        # Adaptive average pooling\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # N x 768 x 1 x 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        # N x 768\n",
        "        x = self.fc(x)\n",
        "        # N x 1000\n",
        "        return x\n",
        "\n",
        "\n",
        "    class BasicConv2d(nn.Module):\n",
        "\n",
        "      def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "1a922469-0266-4c20-d1ae-d8656e4d108e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "4f9a4588-bac8-4815-c94e-3ead0b51e6ab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "9db1bb78-9532-48de-b053-aeed7407de19"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5624 - accuracy: 0.5655\n",
            "Epoch 1: val_loss improved from inf to 6.96322, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 498ms/step - loss: 2.5624 - accuracy: 0.5655 - val_loss: 6.9632 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9127\n",
            "Epoch 2: val_loss improved from 6.96322 to 2.64265, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 602ms/step - loss: 0.2712 - accuracy: 0.9127 - val_loss: 2.6426 - val_accuracy: 0.2333\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9722\n",
            "Epoch 3: val_loss improved from 2.64265 to 0.83634, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 482ms/step - loss: 0.0843 - accuracy: 0.9722 - val_loss: 0.8363 - val_accuracy: 0.7222\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9940\n",
            "Epoch 4: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 8s 479ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 1.4221 - val_accuracy: 0.5222\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9921\n",
            "Epoch 5: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 8s 488ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 0.9192 - val_accuracy: 0.7667\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 7s 469ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 1.4963 - val_accuracy: 0.6000\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9960\n",
            "Epoch 7: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 7s 467ms/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 1.8705 - val_accuracy: 0.6111\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9980\n",
            "Epoch 8: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 7s 459ms/step - loss: 0.0035 - accuracy: 0.9980 - val_loss: 1.4575 - val_accuracy: 0.6333\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 8s 471ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.1539 - val_accuracy: 0.5333\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 10: val_loss did not improve from 0.83634\n",
            "16/16 [==============================] - 9s 577ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.1000 - val_accuracy: 0.7667\n",
            "7/7 [==============================] - 1s 146ms/step - loss: 0.5481 - accuracy: 0.8884\n",
            "Test accuracy: 88.84%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2GLB0e86Dx2e2737x9QCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "cfa43dca-e786-4d24-a08c-a9e5d095f97c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "eb033590-5438-43eb-ef1b-f52d6fc01396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "78f89888-19c4-46be-c6ab-073b9ef9e130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "d991e240-bc7e-4a03-b3a1-cc83a0e4521e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1582  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1583  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1584  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1585  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1586  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "3afd24c5-90ae-4e79-8d0b-a42f070731ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "534  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "535  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "536  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "537  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "538  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "f49d321f-b61e-4885-9100-4686748e886a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1587\n",
            "Total videos for testing: 539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "633          633  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "1388        1388  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   26\n",
              "274          274  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   86\n",
              "993          993  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   39\n",
              "1129        1129  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    3\n",
              "883          883  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "814          814  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   52\n",
              "231          231  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   82\n",
              "691          691  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   55\n",
              "712          712  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>1388</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>814</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>231</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>691</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>712</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\"))\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "6e36ba69-3d66-45fd-a94d-4d693fda330e"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "431d8796-9102-49da-a85b-84ef0e893ac8"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "e9a21c6a-0e88-41b2-b454-8909e3bf8a8d"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3974 - accuracy: 0.7282\n",
            "Epoch 1: val_loss improved from inf to 0.61570, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 491ms/step - loss: 1.3974 - accuracy: 0.7282 - val_loss: 0.6157 - val_accuracy: 0.6889\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9901\n",
            "Epoch 2: val_loss did not improve from 0.61570\n",
            "16/16 [==============================] - 7s 453ms/step - loss: 0.0315 - accuracy: 0.9901 - val_loss: 1.0080 - val_accuracy: 0.5556\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9940\n",
            "Epoch 3: val_loss did not improve from 0.61570\n",
            "16/16 [==============================] - 7s 452ms/step - loss: 0.0209 - accuracy: 0.9940 - val_loss: 1.4756 - val_accuracy: 0.4889\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9960\n",
            "Epoch 4: val_loss improved from 0.61570 to 0.40763, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 464ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.4076 - val_accuracy: 0.8333\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9980\n",
            "Epoch 5: val_loss did not improve from 0.40763\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.6121 - val_accuracy: 0.8222\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.40763\n",
            "16/16 [==============================] - 7s 445ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6658 - val_accuracy: 0.8000\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9980\n",
            "Epoch 7: val_loss did not improve from 0.40763\n",
            "16/16 [==============================] - 7s 446ms/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 0.4445 - val_accuracy: 0.8667\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 0.3848 - accuracy: 0.9196\n",
            "Test accuracy: 91.96%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
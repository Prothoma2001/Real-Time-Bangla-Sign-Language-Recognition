{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiyMxEoS5kQxSTBRrpC+cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "5243b2f1-035f-4c7d-8020-124a68b45c54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "894016fa-d629-4767-ad42-48db8d5fd8f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "1064fc63-56d2-4a5a-af89-6e260255db97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "184ae951-aa0d-485c-d50d-cb5d8906d127"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1482  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1483  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1484  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1485  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1486  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "7443004e-6350-4eea-d7d3-2e2432b7b284"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  96  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "434  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "435  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "436  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "437  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "438  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "79abd34d-35a9-46f9-c9db-7b6ad3b10da1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1487\n",
            "Total videos for testing: 439\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "575          575  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   71\n",
              "1478        1478  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   17\n",
              "832          832  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   49\n",
              "1267        1267  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   24\n",
              "5              5  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   92\n",
              "406          406  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   76\n",
              "485          485  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   63\n",
              "1389        1389  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   16\n",
              "530          530  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   70\n",
              "1044        1044  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   34"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0eb1568-e575-4087-9681-95a7fe345643\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>575</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>1478</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>832</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>1267</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>406</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>485</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>1389</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>530</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>1044</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0eb1568-e575-4087-9681-95a7fe345643')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0eb1568-e575-4087-9681-95a7fe345643 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0eb1568-e575-4087-9681-95a7fe345643');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "bdf598ea-cf39-4815-e3b5-357547177b41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "deb23b65-f809-41c4-e187-1fb3c01e9cfe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "8dd600ea-7648-4e65-9b64-b137d5819490"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7683 - accuracy: 0.5159\n",
            "Epoch 1: val_loss improved from inf to 5.42386, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 10s 530ms/step - loss: 2.7683 - accuracy: 0.5159 - val_loss: 5.4239 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.8869\n",
            "Epoch 2: val_loss improved from 5.42386 to 1.55787, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.2896 - accuracy: 0.8869 - val_loss: 1.5579 - val_accuracy: 0.2889\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9603\n",
            "Epoch 3: val_loss improved from 1.55787 to 0.49770, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.0919 - accuracy: 0.9603 - val_loss: 0.4977 - val_accuracy: 0.8000\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 4: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 10s 603ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.7778\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9960\n",
            "Epoch 5: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 13s 783ms/step - loss: 0.0099 - accuracy: 0.9960 - val_loss: 1.0513 - val_accuracy: 0.6889\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9960\n",
            "Epoch 6: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 8s 511ms/step - loss: 0.0072 - accuracy: 0.9960 - val_loss: 0.6205 - val_accuracy: 0.8222\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 8s 486ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1496 - val_accuracy: 0.6778\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 8: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 8s 484ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6516 - val_accuracy: 0.8444\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 8s 489ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0255 - val_accuracy: 0.7444\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 10: val_loss did not improve from 0.49770\n",
            "16/16 [==============================] - 8s 484ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3291 - val_accuracy: 0.6333\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 0.3450 - accuracy: 0.9152\n",
            "Test accuracy: 91.52%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFP5vKt2mzomDb51pJe4I6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "cfa43dca-e786-4d24-a08c-a9e5d095f97c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "eb033590-5438-43eb-ef1b-f52d6fc01396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "78f89888-19c4-46be-c6ab-073b9ef9e130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "d991e240-bc7e-4a03-b3a1-cc83a0e4521e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1582  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1583  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1584  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1585  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1586  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "3afd24c5-90ae-4e79-8d0b-a42f070731ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "534  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "535  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "536  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "537  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "538  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "f49d321f-b61e-4885-9100-4686748e886a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1587\n",
            "Total videos for testing: 539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "633          633  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "1388        1388  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   26\n",
              "274          274  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   86\n",
              "993          993  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   39\n",
              "1129        1129  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    3\n",
              "883          883  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "814          814  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   52\n",
              "231          231  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   82\n",
              "691          691  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   55\n",
              "712          712  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>1388</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>814</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>231</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>691</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>712</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "2c0f1652-2696-4970-8b8c-842cf13e890d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "8e9b2261-5ee2-4ac6-944b-f18013a14d9d"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "d020b5ae-bb0d-414f-8e10-bed77488ea70"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4160 - accuracy: 0.6032\n",
            "Epoch 1: val_loss improved from inf to 7.73660, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 493ms/step - loss: 2.4160 - accuracy: 0.6032 - val_loss: 7.7366 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9504\n",
            "Epoch 2: val_loss improved from 7.73660 to 0.98975, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 474ms/step - loss: 0.1402 - accuracy: 0.9504 - val_loss: 0.9898 - val_accuracy: 0.5222\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9921\n",
            "Epoch 3: val_loss improved from 0.98975 to 0.69110, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 467ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 0.6911 - val_accuracy: 0.7556\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 4: val_loss did not improve from 0.69110\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.7859 - val_accuracy: 0.7444\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980\n",
            "Epoch 5: val_loss did not improve from 0.69110\n",
            "16/16 [==============================] - 7s 448ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.8060 - val_accuracy: 0.7444\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.69110\n",
            "16/16 [==============================] - 7s 453ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.0866 - val_accuracy: 0.6778\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.69110\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8581 - val_accuracy: 0.7556\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 0.6115 - accuracy: 0.9062\n",
            "Test accuracy: 90.62%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
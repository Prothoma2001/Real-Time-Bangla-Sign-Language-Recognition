{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh9nfMeqEq0Vbae2BIPQty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "8ec6383a-1bdf-4820-b6f0-4987263a07c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "21956ff8-d55e-4e8a-c6d3-9f7eb9cb57bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "72073c4c-05a0-4f47-e86b-c08c76c5e8c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "897d0ea7-701d-4fba-99e1-f8ccc0c4615b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "2087  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2088  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2089  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2090  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2091  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "10638e93-902f-47a0-f944-aaacac5e3516"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "737  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "738  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "739  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "740  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "741  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "b0eb1426-36cb-4b79-9141-41674c55e0ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 2092\n",
            "Total videos for testing: 742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "436          436  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   77\n",
              "162          162  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   95\n",
              "400          400  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    9\n",
              "428          428  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   73\n",
              "1758        1758  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   23\n",
              "1161        1161  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "1221        1221  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   51\n",
              "1125        1125  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   50\n",
              "1638        1638  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   27\n",
              "1727        1727  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a7e3080-8b93-4516-a66c-b68b6e72c0c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>436</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>162</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>400</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>428</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1758</th>\n",
              "      <td>1758</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>1161</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>1221</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1125</th>\n",
              "      <td>1125</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>1638</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1727</th>\n",
              "      <td>1727</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a7e3080-8b93-4516-a66c-b68b6e72c0c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a7e3080-8b93-4516-a66c-b68b6e72c0c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a7e3080-8b93-4516-a66c-b68b6e72c0c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\")) #base_filter=64\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) \n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=128\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=256\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "863fae1b-d06c-4e8d-d6d9-f0e4b52d7b1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "ac31d145-74ba-4f51-b81e-b904d201c492"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)  #base_dropout=0.5\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "79c33cd6-6d71-4805-ef0e-c2187e791aaa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5891 - accuracy: 0.5714\n",
            "Epoch 1: val_loss improved from inf to 8.67134, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 483ms/step - loss: 2.5891 - accuracy: 0.5714 - val_loss: 8.6713 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9187\n",
            "Epoch 2: val_loss improved from 8.67134 to 3.22499, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 485ms/step - loss: 0.2637 - accuracy: 0.9187 - val_loss: 3.2250 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9742\n",
            "Epoch 3: val_loss improved from 3.22499 to 0.58466, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 491ms/step - loss: 0.0668 - accuracy: 0.9742 - val_loss: 0.5847 - val_accuracy: 0.7333\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9940\n",
            "Epoch 4: val_loss improved from 0.58466 to 0.21928, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.0228 - accuracy: 0.9940 - val_loss: 0.2193 - val_accuracy: 0.9111\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9940\n",
            "Epoch 5: val_loss did not improve from 0.21928\n",
            "16/16 [==============================] - 8s 491ms/step - loss: 0.0140 - accuracy: 0.9940 - val_loss: 0.7261 - val_accuracy: 0.7111\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 0.21928\n",
            "16/16 [==============================] - 8s 489ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.2268 - val_accuracy: 0.9111\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.21928\n",
            "16/16 [==============================] - 8s 471ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 0.7889\n",
            "7/7 [==============================] - 1s 130ms/step - loss: 0.2662 - accuracy: 0.9196\n",
            "Test accuracy: 91.96%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRUVbbA4d8mILMyizIIKiAihiEMMig4ga0GFRVi6xJRsFGbwW67lW6FRl2trd0P6bbxgagPNQmggqgogoCiIBJGmUGMEEWEMAgGJEnt98dJQhIykVRy61btb61aSVXdundXVbLr1LnnnC2qijHGGP+r5HUAxhhjgsMSujHGhAlL6MYYEyYsoRtjTJiwhG6MMWGislcHbtCggbZo0cKrwxtjjC+tWrVqv6o2LOg+zxJ6ixYtSEpK8urwxhjjSyLyXWH3WZeLMcaECUvoxhgTJiyhG2NMmLCEbowxYcISujHGhIliE7qIvCIiP4nIhkLuFxGZJCI7RGS9iHQKfpjGGGOKU5IW+mtA/yLuvw5olXUZDkwue1jGGGNOV7Hj0FX1MxFpUcQmA4Dp6tbh/VJE6ojIOaq6J0gxGmMigSpkZkJGRsGXzEwIBIq+qBa/TShcbrwRunQJ+ksYjIlFTYDdua6nZN12SkIXkeG4VjzNmzcPwqGNCYJff4XUVNi/310OHnSJIRwEAkUnyMLuC+alpMfJzPT61aoYInDuuSGb0EtMVacAUwBiYmLC5D/GhJQTJ/Im5/37T72e/7ajR72OOnRUrly2S40aJ3+Piir7/vLvq1KlUy8iBd8eihcRdymvty8I+/geaJbretOs24wpm/T0vIm3uMS8fz8cOVL4/s48E+rXhwYNoGFDaNv25PXclzp1XAIJByJQpUrJkmx2wjG+FYyEPhd4SEQSgW7AYes/N6dIT4cDB0qemFNT4fDhwvdXq1beJNy69amJOXeyrl8fzjij4p6vMR4oNqGLSALQB2ggIinAOKAKgKq+BMwDfgPsANKAe8orWBOCAgHYtg2++gq+/77wZH3oUOH7qFkzb+K98MLCE3P29apVK+45GuMTJRnlElfM/Qo8GLSITGjbvx9WrHCXL790iTx3S7pGjbyJ9/zzC0/M2bdVq+bd8zEmjHi2fK7xgRMnYO3ak8l7xQr45ht3X6VK0L49DB4M3bq5S4sWLqEbYzxhCd04qpCcnDd5r1njhvSBG2bVvTsMH+6Sd+fOrh/bGBMyLKFHqp9/hpUr8ybwn35y91WvDjEx8Pvfu+TdvTs0beptvMaYYllCjwSZmbBxY97kvWnTyckzF10E1113Mnlfcokb6maM8RVL6OFoz568yXvlSvjlF3dfvXouad9+u/vZpQvUrettvMaYoLCE7nfHjsHq1SeT94oVsGuXu69KFejQAe65xyXvbt3gggts8ogxYcoSup+owvbteZP3unVuHQxwo0wuuwxGj3YJvGNHGxJoTASxhB7KDhxw47xzJ/CDB919tWpB167wyCMnW99nn+1tvMYYT1lCDxXp6bB+/cnk/eWXrjUOrovkkktg4MCTybtt2/BZb8QYExSW0L20bh1Mn+6S9+rVcPy4u71xY5e0hw51P2NioHZtb2M1xoQ8S+he2bUL+vaFtDQ3SeeBB07OuGze3E5cGmNOmxWJ9kJ6OsTFuZOZGzbAF1/AP//phhKed54lc48EAvDvf7uFGxMTvY7GZNu927V9rroKFiwIn9oj5cESuhfGjYNly2DKFLeyoPFcSgr06wcjR7qFIePiYMwY99lrvPPJJ9CpE6xaBVu2wLXXurEAs2e7D2CTlyX0irZgATzzDNx3n1vYynguIcGtM7ZsGbz0kkvuI0fCxIlw5ZVunpapWKru3+Taa6FRIzc3budO1wY6eBBuucWNE3j9dfvQzUNVPbl07txZI86ePaqNGqm2a6f6yy9eRxPxUlNVBw1SBdXu3VW3b897f3y8ao0aqmefrfrpp97EGIkOHVK96Sb3vgwapHrkSN7709Pde9O+vdvmvPNUX3xRNS3Nk3ArHJCkheRVS+gVJSND9aqrVKtXV92wwetoIt5HH6mee65q5cqqTz3lkkRBvv5atVUr1ago1X/+UzUQqNg4I8369e71rlxZdeLEol/vQED1vffchzG4D95nn1U9fLji4vWCJfRQ8NRT7uV++WWvI4lov/yi+uCD7q24+GLVVauKf8yhQ6o33+wec9ttqj//XP5xRqI333TfiBo3Vl26tOSPCwRUFy9WveYa9x7VqaP6+OOq+/aVW6iesoTutaVLVStVUo2Lsyaeh776SrV1a/dXP3r06X1FDwRc669SJdW2bVU3bSq/OCPNr7+q/v737n3p3Vv1hx9Kv6+vvjr54VujhuqYMaopKcGLNRRYQvfS/v2qTZuqXnBB+H8XDFEnTqiOG+e6TZo1U/3kk9Lv65NPVBs2VK1VS3XWrKCFGLFSUlQvu8xlojFj3HsVDBs2qN51l3vPq1RRHTbs1HMkfmUJ3SuBgGpsrPuLSkryOpqItGWLapcu7i/9zjtVDx4s+z5371bt1s3t8w9/KLz/3RRt8WI3RqBmTdUZM8rnGDt3qo4YoVq16skvyevWlc+xKooldK9MnOhe4okTvY4k4gQCqv/+tzsHXa9e8FvTx4+rPvCAe3uvuMINYDIlEwioPvecaz1fdJHqxo3lf8wfflB95BH3zQpUb7xRdfny8j9uebCE7oWVK13LPDbW+s0rWEqK6rXXur/u/v3L1idbnOnT3YfGOeeofv55+R0nXBw+rDpwoHtvbr214k8wHzigOmGC+5AH1b59VT/+2F//opbQK9rhw67PvFkzN9jZVJjERNW6dd0JscmTK+Yfdd0693ZXrqz6wgv+Sg4VaeNG1TZtXMv8+ee9fZ2OHFH917/c0FVQjYlRfecd1cxM72IqKUvoFSkQUB082P3VWpOtwhw44PpHwfVvb9tWscc/eNB9jQcXx9GjFXv8UDdjhusrb9RIdckSr6M56fhx1SlT3Ady9lDW6dODd3K2PFhCr0hTp7qX9emnvY4kYnz8sWqTJq6F/OST3p2kzMx0b3ulSm4y8Nat3sQRSk6ccENEQbVHj9AdQpg9+/SSS1ysLVqo/ve/qseOeR3ZqSyhV5Svv3Ydqldf7Y/vbj73yy8nxy9fdFHoDCT6+GPV+vVVa9d2X+Mj1Q8/qPbq5d6fkSPdePNQl5mpOnfuydmnjRur/uMfoTWZzBJ6RfjlF/d97eyzbchDBfjqK9cfC6qjRoXeOh7ffXdyuOSf/xx5Qxs/+8wlwxo1XMvXbwqaffrEE25aidcsoVeEe+9VFVFdsMDrSMJaerrq3/7mTlE0bRraL/fx46r33685oyn27vU6ovIXCKj+z/+496dVK/el1e9yzz6tWVP14Ye97TqyhF7e4uPdSzl2rNeRhLWtW1W7dnUv9W9/G5xJQhXh1VdVq1Vz/fx+HftcEkeOnFy98qab3Bo44ST37NMzzvBu9qkl9PK0fbubrdCzZ+R9r64ggYBbHrV6dTcksbxmFZanNWtUW7Z0UxNefDH8hjZu3uzWuKlUSfWZZ8Lv+eWWf/bpHXe4VSIriiX08nL8uGqnTm6Wwq5dXkcTlr7/XrVfP/eX2q+fu+5XBw6o/uY3mrMMQbgsif/WW65N07Bh2dbJ8RuvZp+WOaED/YGtwA7g0QLubw4sBtYA64HfFLfPsEjoI0e6l/Ddd72OJCzNmOFa5NWrh0+rNjPTzVQUUb30Un8vGJWervrHP2rO2P/du72OyBsFzT5dsKD8/l7LlNCBKOAb4HzgDGAdcHG+baYAI7J+vxhILm6/vk/oc+ZozhALE1QHDrivseD6zMNxPPeHH7oEcNZZ/mwP/Pijap8+7j164AH3ZTXS5Z992qWL6uzZwR/BXFRCL0lN0a7ADlXdqaongERgQP5KdsCZWb+fBfxQgv36165dcM89rnrts896HU1YWbjQ1fecMQP+9jf44gto3drrqIKvf39X+PiCC2DAAPjrXyEz0+uoSmbZMvenv2IFTJ8OL74IVat6HZX3atVyhcWza5+mpsLNN7u/5zfegIyMCgiisEyvJ1vftwIv57p+F/CffNucA3wNpAAHgc6F7Gs4kAQkNW/ePLgfWxUlPd2dAK1d29/fl0NMWtrJHqyLLnJrm0WCY8fciFdwY55DucpOIKA6aZKbkXvBBapr13odUWjLP/u0ZUu3vlBZZ59Sxi6XkiT0h4E/ZP1+GbAJqFTUfn3b5TJ2rHvZEhK8jiRsJCW5JJ49ozDUJglVhKlT3aiJZs1UV6zwOppTHT16shvsxhv9M2Q0FBQ0+/Ttt0u/v6ISekm6XL4HmuW63jTrttzuBWZmtfiXA9WABiX6iuAnCxbA3/8O990Hgwd7HY3vZWTAk09C9+5w5Ah8/DG88AJUr+51ZBXvvvtc91KlStC7N/zv/4JrH3lv2zb3HiUkwFNPwZw5UKeO11H5R6VKcOONrqtq8WLXBdOwYTkdrLBMrydb35WBnUBLTp4UbZdvmw+BIVm/t8X1oUtR+/VdC33PHrdUXLt24TPezENbt56s+hMX506EGje1PHuY5pAh3n9bmT1b9cwz3do08+d7G4txKEsLXVUzgIeA+cBmYKaqbhSRCSISm7XZH4BhIrIOSMhK7iHSvgiCQADuuss1I2fMgBo1vI7It1Rh8mTo2NG1/BITIT4e6tb1OrLQUL8+fPABPPEEvPYa9OjhTrJVtIwMeOwxd1KvdWt3Avfaays+DnOaCsv05X3xVQv96addk2nqVK8j8bXvv3cVhMBVFArVpVRDxfvvu0Wh6tRR/eCDijvu3r2qV17p3qfhw0NzCdlIRhn70CPb55+75lJcHNx7r9fR+NasWa7v8NNP4T//gY8+giZNvI4qtF1/vWsZn3ee+33cuPIf2rhiBXTu7Pp7X3nF9eVXq1a+xzTBYwm9KKmpLpG3aAEvvQQiXkfkO4cOwZ13wu23uzHXa9bAgw/aS1lS558Py5fD3XfDhAlwww3uzzLYsrvCeveGKlVcQr/nnuAfx5QvS+iFUYWhQ2HvXtdvfuaZxT/G5LFokWuVJybC+PFuFEebNl5H5T/Vq8Orr7o2xaJFrgW9alXw9p+WBkOGwAMPwDXXQFKSO8dh/McSemEmTYK5c+G559x/kCmxY8fcjLmrroKaNV0Lc9w41/IzpSMC998PS5e6c/Q9e8K0aWXf7zffwGWXweuvu5m5770H9eqVfb/GG5bQC7JqFTzyCMTGwsiRXkfjK6tXu8+/iRPhoYfc9S5dvI4qfHTt6l7T3r3d2PVhw+D48dLt67333Hu1ezfMm+dOFVWyjOBr9vbl9/PPMGgQNG7svudaZ2+JZGTA009Dt25w+DDMnw///reN8CwPDRq4k8pjx8LLL0OvXpCcXPLHZ2a6tWNiY915jVWr3Noyxv8soeem6r7XJie7aXH23bNEAgE3Rvmvf4Vbb4Wvv7Yxy+UtKsp9gL77Lmzf7lra8+cX/7j9++G669xjhw515zVatiz/eE3FsISe27Rp7gzehAmuk9KUyOefuynN//iHfQ5WtNhY18Ju0sQl6iefdB+wBVm50iX+zz6DqVPdn7sNSQwvltCzbdzo+suvvhoefdTraHwlIcF1rYwY4XUkkenCC+HLL+G3v3X94LGxcPDgyftVXQLv1ctd//xz1/9uwo8ldHDjtm6/3Q1NfP11OzN0GtLT3aSh2Fi3HrTxRo0abm3y//zHLXIWEwNr17oRR/feC8OHQ58+rjUfE+N1tKa8WOYCGDUKNm92q9A3bux1NL6yYIGb6HLHHV5HYkTcpK3PPoNff3XDETt3duf2H3/cjWRpEH5roJpcLKEnJLihAo895rpbzGnJXlirXz+vIzHZund3Qxt79IA9e9zwxAkT3IlUE94qex2Ap3bscN9Fe/Z0syrMaUlLc2tj33EHnHGG19GY3Bo1cuX8Tpyw8nCRJHJb6L/+6sabV6nimpmVI/uzrTTeew9++cW6W0KViCXzSBO5WezPf3bfS999F5o39zoaX4qPh3PPdbMWjTHei8wW+ty5rtbZqFFueIY5bQcPwocfukp81jdrTGiIvIS+a5dbWq5TJ3j2Wa+j8a2333ZDFq27xZjQEVkJPSPDZaCMDLckrnUwllp8PLRq5T4XjTGhIbIS+rhxbvGKKVPc9DpTKj/8AEuWuM9GW7vMmNAROQl94UL4+9/dnOfBg72OxtdmzHDTyePivI7EGJNbZCT0H390ddAuvtidDDVlkpDgulqs+pAxoSX8E3ogAHfd5dY5nzHDFuguo+3b3ap91jo3JvSE/zj0Z55x3S1Tp0K7dl5H43uJia7f3HqtjAk94d1C/+ILt55oXJxbcs6UiSq8+SZcfjk0bep1NMaY/MI3oaemukTeooUrl27DMcps7VrYutW6W4wJVeHZ5aLq6mv9+KMrOX/mmV5HFBYSEtySN7fe6nUkxpiChGdCnzTJTe+fONEtCG3KLBBwCb1fP6hf3+tojDEFCb8ul1Wr4JFH3BotI0d6HU3Y+OILSEmxqf7GhLLwSug//+yWxG3c2JVpsX7zoImPdyM+bS0zY0JX+HS5qML990NyMnz6qZWeDyKrG2qMP5SohS4i/UVkq4jsEJFHC9nmdhHZJCIbRSQ+uGGWwLRpbpD0hAmuApEJGqsbaow/FNtCF5Eo4EXgGiAFWCkic1V1U65tWgGPAT1V9aCINCqvgAu0caPrL7/6ani0wM8bUwZWN9QYfyhJC70rsENVd6rqCSARGJBvm2HAi6p6EEBVfwpumEVIS4Pbb3dDE19/HSqF12kBr2XXDb31VqsbakyoK0n2awLsznU9Jeu23FoDrUXkCxH5UkT6F7QjERkuIkkikrRv377SRZzfqFGweTO88YY7GWqCyuqGGuMfwWrOVgZaAX2AOGCqiNTJv5GqTlHVGFWNadiwYdmPmpAAL78Mjz3multM0CUkWN1QY/yiJAn9e6BZrutNs27LLQWYq6rpqvotsA2X4MvPjh0wfLg7Afq3v5XroSLVwYMwb57VDTXGL0qS0FcCrUSkpYicAQwG5ubbZg6udY6INMB1wewMYpx5/fqrG29epYo7Y1c5fEZfhpLsuqG2dosx/lBsJlTVDBF5CJgPRAGvqOpGEZkAJKnq3Kz7rhWRTUAm8IiqppZb1H/+M6xeDe++C82bl9thIl1CgqsbaqsnGOMPJWraquo8YF6+257I9bsCD2ddytfcua7q0KhRNm2xHP3wAyxeDI8/bhNujfEL/43xq1QJrrkGnn3W60jC2syZVjfUGL8R17iueDExMZqUlFS6B6tas7Gcde0KmZlurTNjTOgQkVWqGlPQff5roYMl83JmdUON8Sd/JnRTrqxuqDH+ZAnd5KHqRoL27m11Q43xG0voJo9162DLFpvqb4wfWUI3eWTP07K6ocb4jyV0kyMQcP3nVjfUGH+yhG5yfPEF7N5t3S3G+JUldJMjPh6qV7cJuMb4lSV0A5ysGzpggNUNNcavLKEb4GTdUJtMZIx/WUI3gFtZsW5d6F9grSljjB9YQjekpcHs2VY31Bi/s4RueP99VzfUuluM8TdL6Ib4eFc39PLLvY7EGFMWltAjXHbd0EGDrG6oMX5nCT3CvfOOG7Jok4mM8T9L6BEuPt7qhhoTLiyhR7A9e1zd0Lg4qxliTDiwhB7BZsywuqHGhBNL6BEsPh46doSLLvI6EmNMMFhCj1A7dri6oXYy1JjwYQk9QiUkWN1QY8KNJfQIZHVDjQlPltAjkNUNNSY8WUKPQAkJVjfUmHBkCT3CBAIuoVvdUGPCjyX0CJNdN9TGnhsTfiyhR5iEBFc3dMAAryMxxgSbJfQIkp4OM2da3VBjwlWJErqI9BeRrSKyQ0QeLWK7gSKiIhITvBBNsCxcaHVDjQlnxSZ0EYkCXgSuAy4G4kTk4gK2qw2MAlYEO0gTHPHxVjfUmHBWkhZ6V2CHqu5U1RNAIlBQD+yTwLPA8SDGZ4Iku27owIFWN9SYcFWShN4E2J3rekrWbTlEpBPQTFU/KGpHIjJcRJJEJGnfvn2nHawpvey6oTaZyJjwVeaToiJSCfgX8IfitlXVKaoao6oxDRs2LOuhzWmwuqHGhL+SJPTvgWa5rjfNui1bbeASYImIJAPdgbl2YjR0HDwIH35odUONCXclSegrgVYi0lJEzgAGA3Oz71TVw6raQFVbqGoL4EsgVlWTyiVic9reeQdOnLDuFmPCXbEJXVUzgIeA+cBmYKaqbhSRCSISW94BmrJLSIALL7S6ocaEu8ol2UhV5wHz8t32RCHb9il7WCZY9uyBRYvg8cetbqgx4c5mioY5qxtqTOSwhB7mEhKsbqgxkcISehjbsQO++spOhhoTKSyhh7HERPdz0CBv4zDGVAxL6GFKFd58000katas+O2NMf5nCT1MZdcNtZOhxkQOS+hhyuqGGhN5LKGHodx1Qxs08DoaY0xFsYQehpYts7qhxkQiS+hhKD7e6oYaE4ksoYeZ9HSYNQtiY61uqDGRxhJ6mFm4EPbvt8lExkQiS+hhxuqGGhO5LKGHkbQ0mDPH6oYaE6ksoYeR99+Ho0etu8WYSGUJPYwkJMA551jdUGMilSX0MHHoEMybB4MHW91QYyKVJfQwYXVDjTGW0MNEfLzVDTUm0llCDwPZdUPvuMPqhhoTySyhh4GZM61uqDHGEnpYiI+3uqHGGEvovvfNN65uqLXOjTGW0H0uIcH9HDzY2ziMMd6zhO5jqq67xeqGGmPAErqvrV8Pmzdbd4sxxrGE7mPx8VY31BhzkiV0nwoEIDERrr3W6oYaYxxL6D61bBns2mVT/Y0xJ1lC96mEBKsbaozJq0QJXUT6i8hWEdkhIo8WcP/DIrJJRNaLyCcicl7wQzXZ0tPd7FCrG2qMya3YhC4iUcCLwHXAxUCciFycb7M1QIyqXgq8Bfwj2IGak6xuqDGmICVpoXcFdqjqTlU9ASQCeb7oq+piVU3Luvol0DS4YZrcEhKgTh3o18/rSIwxoaQkCb0JsDvX9ZSs2wpzL/BhQXeIyHARSRKRpH379pU8SpMjLQ1mz3ZDFatW9ToaY0woCepJURG5E4gBnivoflWdoqoxqhrTsGHDYB46YnzwgasbapOJjDH5VS7BNt8DuSeWN826LQ8RuRr4C3CFqv4anPBMfvHxrm7oFVd4HYkxJtSUpIW+EmglIi1F5AxgMDA39wYi0hH4XyBWVX8KfpgGrG6oMaZoxSZ0Vc0AHgLmA5uBmaq6UUQmiEhs1mbPAbWAWSKyVkTmFrI7UwbZdUOtu8UYUxBRVU8OHBMTo0lJSZ4c26+uvhq++w62bbNSc8ZEKhFZpaoxBd1nM0V9Ys8eWLzYtc4tmRtjCmIJ3SdmznQLcll3izGmMJbQfSIhwdUNbdvW60iMMaHKEroPfPMNrFhhrXNjTNEsofuA1Q01xpSEJfQQl103tHdvqxtqjClaSWaKGg9l1w2dPNnrSEx5SU9PJyUlhePHj3sdigkh1apVo2nTplSpUqXEj7GEHuISEqxuaLhLSUmhdu3atGjRArExqQZQVVJTU0lJSaFly5Ylfpx1uYSwQMAldKsbGt6OHz9O/fr1LZmbHCJC/fr1T/tbmyX0EGZ1QyOHJXOTX2n+JiyhhzCrG2qMOR2W0EOU1Q01FSU1NZUOHTrQoUMHGjduTJMmTXKunzhxosjHJiUlMXLkyGKP0aNHj2CFC8Do0aNp0qQJgUAgqPv1OzspGqI++cTVDbXJRKa81a9fn7Vr1wIwfvx4atWqxR//+Mec+zMyMqhcueBUERMTQ0xMgetE5bFs2bLgBAsEAgFmz55Ns2bN+PTTT+nbt2/Q9p1bUc87VPkr2ggSH+/qhvbv73UkpkKNHg1ZyTVoOnSAiRNP6yFDhgyhWrVqrFmzhp49ezJ48GBGjRrF8ePHqV69Oq+++ipt2rRhyZIlPP/887z//vuMHz+eXbt2sXPnTnbt2sXo0aNzWu+1atXi6NGjLFmyhPHjx9OgQQM2bNhA586deeONNxAR5s2bx8MPP0zNmjXp2bMnO3fu5P333z8ltiVLltCuXTsGDRpEQkJCTkLfu3cvv/vd79i5cycAkydPpkePHkyfPp3nn38eEeHSSy/l9ddfZ8iQIdxwww3cmjV8LHd8jz/+OHXr1mXLli1s27aNm266id27d3P8+HFGjRrF8OHDAfjoo48YO3YsmZmZNGjQgAULFtCmTRuWLVtGw4YNCQQCtG7dmuXLl1NRFdosoYegY8dc3dDBg61uqPFOSkoKy5YtIyoqip9//pmlS5dSuXJlFi5cyNixY3n77bdPecyWLVtYvHgxR44coU2bNowYMeKUcdRr1qxh48aNnHvuufTs2ZMvvviCmJgY7r//fj777DNatmxJXBFfTRMSEoiLi2PAgAGMHTuW9PR0qlSpwsiRI7niiiuYPXs2mZmZHD16lI0bN/LUU0+xbNkyGjRowIEDB4p93qtXr2bDhg05wwVfeeUV6tWrx7Fjx+jSpQsDBw4kEAgwbNiwnHgPHDhApUqVuPPOO3nzzTcZPXo0CxcuJDo6usKSOVhCD0nvv291QyPWabaky9Ntt91GVFZprMOHD3P33Xezfft2RIT09PQCH3P99ddTtWpVqlatSqNGjdi7dy9NmzbNs03Xrl1zbuvQoQPJycnUqlWL888/PyeJxsXFMWXKlFP2f+LECebNm8e//vUvateuTbdu3Zg/fz433HADixYtYvr06QBERUVx1llnMX36dG677TYaZI37rVevXrHPu2vXrnnGfk+aNInZs2cDsHv3brZv386+ffu4/PLLc7bL3u/QoUMZMGAAo0eP5pVXXuGee+4p9njBZAk9BFndUBMKatasmfP7448/Tt++fZk9ezbJycn06dOnwMdUzfWVMioqioyMjFJtU5j58+dz6NAh2rdvD0BaWhrVq1fnhhtuKPE+ACpXrpxzQjUQCOQ5+Zv7eS9ZsoSFCxeyfPlyatSoQZ8+fYocG96sWTPOPvtsFi1axFdffcWbb755WnGVlY1yCTHZdUMHDbK6oSZ0HD58mCZNmkWZWeMAAAvHSURBVADw2muvBX3/bdq0YefOnSQnJwMwY8aMArdLSEjg5ZdfJjk5meTkZL799lsWLFhAWloaV111FZOz1sjIzMzk8OHDXHnllcyaNYvU1FSAnC6XFi1asGrVKgDmzp1b6DeOw4cPU7duXWrUqMGWLVv48ssvAejevTufffYZ3377bZ79Atx3333ceeedeb7hVBRL6CEmu26oTSYyoeRPf/oTjz32GB07djytFnVJVa9enf/+97/079+fzp07U7t2bc4666w826SlpfHRRx9x/fXX59xWs2ZNevXqxXvvvccLL7zA4sWLad++PZ07d2bTpk20a9eOv/zlL1xxxRVER0fz8MMPAzBs2DA+/fRToqOjWb58eZ5WeW79+/cnIyODtm3b8uijj9K9e3cAGjZsyJQpU7jllluIjo5m0KBBOY+JjY3l6NGjFd7dAlZTNORccw0kJ1vd0EiyefNm2lrlEo4ePUqtWrVQVR588EFatWrFmDFjvA7rtCUlJTFmzBiWLl1a5n0V9LdhNUV94scfYdEiqxtqItPUqVPp0KED7dq14/Dhw9x///1eh3TannnmGQYOHMjf//53T45vLfQQMmkSjBoFmzZZqblIYi10UxhroftYfLybA2L/28aY0rCEHiKy64bayVBjTGlZQg8RiYnup9UNNcaUliX0EGB1Q40xwWAJPQR8/bU7EWrdLcYLffv2Zf78+XlumzhxIiNGjCj0MX369CF7UMNvfvMbDh06dMo248eP5/nnny/y2HPmzGHTpk0515944gkWLlx4OuEXKdKW2bWEHgLi461uqPFOXFwcidl9flkSExOLXCArt3nz5lGnTp1SHTt/Qp8wYQJXX311qfaVX/5ldstLeUy0Ki1L6B6zuqEmt9GjoU+f4F5Gjy76mLfeeisffPBBznomycnJ/PDDD/Tu3ZsRI0YQExNDu3btGDduXIGPb9GiBfv37wfg6aefpnXr1vTq1YutW7fmbDN16lS6dOlCdHQ0AwcOJC0tjWXLljF37lweeeQROnTowDfffMOQIUN46623APjkk0/o2LEj7du3Z+jQofz66685xxs3bhydOnWiffv2bNmypcC4spfZHTFiBAkJCTm37927l5tvvpno6Giio6Nz1mqfPn06l156KdHR0dx1110AeeIBt8xu9r579+5NbGwsF198MQA33XQTnTt3pl27dnkWFvvoo4/o1KkT0dHRXHXVVQQCAVq1asW+ffsA98Fz4YUX5lwvC0voHlu+3NUNtZUVjVfq1atH165d+fDDDwHXOr/99tsREZ5++mmSkpJYv349n376KevXry90P6tWrSIxMZG1a9cyb948Vq5cmXPfLbfcwsqVK1m3bh1t27Zl2rRp9OjRg9jYWJ577jnWrl3LBRdckLP98ePHGTJkCDNmzODrr78mIyMjZ50WgAYNGrB69WpGjBhRaLdO9jK7N998Mx988EHOei3Zy+yuW7eO1atX065du5xldhctWsS6det44YUXin3dVq9ezQsvvMC2bdsAt8zuqlWrSEpKYtKkSaSmprJv3z6GDRvG22+/zbp165g1a1aeZXaBoC6za6steiw+3uqGmpO8Wj03u9tlwIABJCYmMm3aNABmzpzJlClTyMjIYM+ePWzatIlLL720wH0sXbqUm2++mRo1agBuTZNsGzZs4K9//SuHDh3i6NGj9OvXr8h4tm7dSsuWLWndujUAd999Ny+++CKjs75u3HLLLQB07tyZd95555THR+oyuyVK6CLSH3gBiAJeVtVn8t1fFZgOdAZSgUGqmhyUCMNYejrMmuXqhtau7XU0JpINGDCAMWPGsHr1atLS0ujcuTPffvstzz//PCtXrqRu3boMGTKkyKVjizJkyBDmzJlDdHQ0r732GkuWLClTvNlL8Ba2/G6kLrNbbJeLiEQBLwLXARcDcSJycb7N7gUOquqFwP8AzwYlujD3ySewb591txjv1apVi759+zJ06NCck6E///wzNWvW5KyzzmLv3r05XTKFufzyy5kzZw7Hjh3jyJEjvPfeezn3HTlyhHPOOYf09PQ8yat27docOXLklH21adOG5ORkduzYAcDrr7/OFadRICBSl9ktSQu9K7BDVXcCiEgiMADYlGubAcD4rN/fAv4jIqLlsFDMK6/AP/8Z7L16Y98+qxtqQkd2f3P2iJfo6Gg6duzIRRddRLNmzejZs2eRj+/UqRODBg0iOjqaRo0a0aVLl5z7nnzySbp160bDhg3p1q1bThIfPHgww4YNY9KkSXlOPlarVo1XX32V2267jYyMDLp06cLvfve7Ej2P7GV2X3rppZzb8i+zO3z4cKZNm0ZUVBSTJ0/msssuy1lmNyoqio4dO/Laa68xbNgwBgwYQHR0NP379y9ymd2XXnqJtm3b0qZNmwKX2Q0EAjRq1IgFCxYArkvqnnvuCeoyu8UuziUitwL9VfW+rOt3Ad1U9aFc22zI2iYl6/o3Wdvsz7ev4cBwgObNm3f+7rvvTjvgd9+FN9447YeFrOuvhyFDvI7CeMkW54pMJVlm93QX56rQk6KqOgWYAm61xdLsY8AAO4FojPG3Z555hsmTJwe9RF1Jhi1+D+SekN4067YCtxGRysBZuJOjxhhj8nn00Uf57rvv6NWrV1D3W5KEvhJoJSItReQMYDAwN982c4G7s36/FVhUHv3nxoQr+3cx+ZXmb6LYhK6qGcBDwHxgMzBTVTeKyAQRyR5oOg2oLyI7gIeBR087EmMiVLVq1UhNTbWkbnKoKqmpqVSrVu20HmcVi4zxWHp6OikpKaUe423CU7Vq1WjatClVqlTJc3vInBQ1xpyqSpUqeWYcGlNatpaLMcaECUvoxhgTJiyhG2NMmPDspKiI7ANOf6qo0wDYX+xW/mDPJfSEy/MAey6hqizP5TxVLXCtXc8SelmISFJhZ3n9xp5L6AmX5wH2XEJVeT0X63IxxpgwYQndGGPChF8T+pTiN/ENey6hJ1yeB9hzCVXl8lx82YdujDHmVH5toRtjjMnHEroxxoQJ3yV0EekvIltFZIeI+HZVRxF5RUR+yqr25Fsi0kxEFovIJhHZKCKjvI6ptESkmoh8JSLrsp7L37yOqaxEJEpE1ojI+17HUhYikiwiX4vIWhHx7ap+IlJHRN4SkS0isllELgvq/v3Uh55VsHobcA2QglurPU5VNxX5wBAkIpcDR4HpqnqJ1/GUloicA5yjqqtFpDawCrjJp++JADVV9aiIVAE+B0ap6pceh1ZqIvIwEAOcqaqnV/I+hIhIMhCTv6yl34jI/wFLVfXlrPoSNVT1ULD277cWek7BalU9AWQXrPYdVf0MOFDshiFOVfeo6uqs34/g1sxv4m1UpaPO0ayrVbIu/mnx5CMiTYHrgZe9jsWAiJwFXI6rH4GqnghmMgf/JfQmwO5c11PwafIIRyLSAugIrPA2ktLL6qJYC/wELFBV3z4XYCLwJyDgdSBBoMDHIrIqq9i8H7UE9gGvZnWDvSwiNYN5AL8ldBOiRKQW8DYwWlV/9jqe0lLVTFXtgKud21VEfNkdJiI3AD+p6iqvYwmSXqraCbgOeDCry9JvKgOdgMmq2hH4hSBXd/NbQi9JwWpTwbL6m98G3lTVd7yOJxiyvgovBvp7HUsp9QRis/qeE4ErReQNb0MqPVX9PuvnT8BsXPer36QAKbm+9b2FS/BB47eEXpKC1aYCZZ1InAZsVtV/eR1PWYhIQxGpk/V7ddzJ9y3eRlU6qvqYqjZV1Ra4/5NFqnqnx2GViojUzDrhTlYXxbWA70aHqeqPwG4RaZN101VAUAcP+KoEnapmiEh2weoo4BVV3ehxWKUiIglAH6CBiKQA41R1mrdRlUpP4C7g66y+Z4CxqjrPw5hK6xzg/7JGU1XCFUT39XC/MHE2MNu1HagMxKvqR96GVGq/B97MapDuBO4J5s59NWzRGGNM4fzW5WKMMaYQltCNMSZMWEI3xpgwYQndGGPChCV0Y4wJE5bQjTEmTFhCN8aYMPH/uq+Ndm9VOPAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
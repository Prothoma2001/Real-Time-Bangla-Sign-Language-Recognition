{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOc8I6GMY662qmeGYeCgGfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "cfa43dca-e786-4d24-a08c-a9e5d095f97c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "eb033590-5438-43eb-ef1b-f52d6fc01396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "78f89888-19c4-46be-c6ab-073b9ef9e130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "d991e240-bc7e-4a03-b3a1-cc83a0e4521e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1582  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1583  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1584  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1585  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1586  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "3afd24c5-90ae-4e79-8d0b-a42f070731ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "534  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "535  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "536  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "537  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "538  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "f49d321f-b61e-4885-9100-4686748e886a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1587\n",
            "Total videos for testing: 539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "633          633  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "1388        1388  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   26\n",
              "274          274  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   86\n",
              "993          993  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   39\n",
              "1129        1129  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    3\n",
              "883          883  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "814          814  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   52\n",
              "231          231  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   82\n",
              "691          691  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   55\n",
              "712          712  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>1388</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>814</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>231</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>691</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>712</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "848de1eb-17ce-4e14-de24-c9809540a10a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "e1bff4f6-bb0f-499e-f88e-5db02d809e37"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "943e3d02-389b-44e4-9f04-79704de15264"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2335 - accuracy: 0.5774\n",
            "Epoch 1: val_loss improved from inf to 2.71137, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 11s 620ms/step - loss: 2.2335 - accuracy: 0.5774 - val_loss: 2.7114 - val_accuracy: 0.0111\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9167\n",
            "Epoch 2: val_loss improved from 2.71137 to 0.47663, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 474ms/step - loss: 0.2154 - accuracy: 0.9167 - val_loss: 0.4766 - val_accuracy: 0.7667\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9861\n",
            "Epoch 3: val_loss did not improve from 0.47663\n",
            "16/16 [==============================] - 7s 457ms/step - loss: 0.0494 - accuracy: 0.9861 - val_loss: 1.1844 - val_accuracy: 0.5889\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9960\n",
            "Epoch 4: val_loss improved from 0.47663 to 0.44529, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 478ms/step - loss: 0.0103 - accuracy: 0.9960 - val_loss: 0.4453 - val_accuracy: 0.8333\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9980\n",
            "Epoch 5: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 453ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 1.3052 - val_accuracy: 0.6667\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 6: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 450ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.1664 - val_accuracy: 0.7000\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 455ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0356 - val_accuracy: 0.7333\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 8: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 450ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7960 - val_accuracy: 0.7889\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3008 - val_accuracy: 0.6889\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 9.4830e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_loss did not improve from 0.44529\n",
            "16/16 [==============================] - 7s 448ms/step - loss: 9.4830e-04 - accuracy: 1.0000 - val_loss: 1.2756 - val_accuracy: 0.7111\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 0.3334 - accuracy: 0.9018\n",
            "Test accuracy: 90.18%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
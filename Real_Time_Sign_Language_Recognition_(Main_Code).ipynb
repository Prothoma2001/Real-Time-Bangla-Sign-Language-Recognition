{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnQuSUYlRCv40R5Tx7a8YZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "id": "ak8iwWqy4W0i"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "d5511978-e1f3-4b7b-ae92-81b07d560a01"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "6eb08520-48b5-4933-bb9a-b2551e23ec53"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "539a1536-8cf0-4a22-9862-6a25aead8571"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "2097  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2098  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2099  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2100  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2101  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "424aa7a8-f291-425e-ba46-69e0a3c6aa47"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "737  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "738  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "739  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "740  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "741  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "442e179f-50e0-48d4-a382-b733d01e3026"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 2102\n",
            "Total videos for testing: 742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "1610        1610  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   35\n",
              "476          476  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   80\n",
              "1360        1360  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   40\n",
              "1870        1870  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   25\n",
              "277          277  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   85\n",
              "1413        1413  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   44\n",
              "1587        1587  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   32\n",
              "618          618  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    7\n",
              "1391        1391  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   37\n",
              "718          718  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   68"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8847c8a0-66e1-4e2b-9049-7037921d3eab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1610</th>\n",
              "      <td>1610</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>476</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>1360</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1870</th>\n",
              "      <td>1870</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>277</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413</th>\n",
              "      <td>1413</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>1587</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>618</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>1391</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>718</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8847c8a0-66e1-4e2b-9049-7037921d3eab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8847c8a0-66e1-4e2b-9049-7037921d3eab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8847c8a0-66e1-4e2b-9049-7037921d3eab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\")) #base_filter=64\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) \n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=128\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=256\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "accb4ae9-4f04-4585-aafd-71c1c7c7af30"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "a1277264-8844-4582-cccc-6f9970adf45d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)  #base_dropout=0.5\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() \n",
        "\n",
        "run_time1 = (time.time()- start_time)\n",
        "print(\"-----%s seconds-----\" % run_time1)"
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "872e350d-50ed-4af9-f4fd-8ff434fdd532"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7941 - accuracy: 0.5456\n",
            "Epoch 1: val_loss improved from inf to 2.94352, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 4s 83ms/step - loss: 2.7941 - accuracy: 0.5456 - val_loss: 2.9435 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/7\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2353 - accuracy: 0.9286\n",
            "Epoch 2: val_loss improved from 2.94352 to 0.42263, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.2128 - accuracy: 0.9365 - val_loss: 0.4226 - val_accuracy: 0.8000\n",
            "Epoch 3/7\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0406 - accuracy: 0.9875\n",
            "Epoch 3: val_loss did not improve from 0.42263\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0394 - accuracy: 0.9881 - val_loss: 0.8656 - val_accuracy: 0.6222\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9980\n",
            "Epoch 4: val_loss did not improve from 0.42263\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.4419 - val_accuracy: 0.8556\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 5: val_loss did not improve from 0.42263\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.8973 - val_accuracy: 0.6667\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 6: val_loss improved from 0.42263 to 0.36187, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.9111\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n",
            "Epoch 7: val_loss did not improve from 0.36187\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.7840 - val_accuracy: 0.7333\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3190 - accuracy: 0.9062\n",
            "Test accuracy: 90.62%\n",
            "-----23.231377363204956 seconds-----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4oexck_nj-5"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}
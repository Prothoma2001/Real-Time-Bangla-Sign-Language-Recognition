{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+TKJWIG0bKd/LA0VNUKra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "cfa43dca-e786-4d24-a08c-a9e5d095f97c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "eb033590-5438-43eb-ef1b-f52d6fc01396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "78f89888-19c4-46be-c6ab-073b9ef9e130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "d991e240-bc7e-4a03-b3a1-cc83a0e4521e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "1582  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1583  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1584  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1585  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1586  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "3afd24c5-90ae-4e79-8d0b-a42f070731ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "534  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "535  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "536  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "537  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "538  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "f49d321f-b61e-4885-9100-4686748e886a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1587\n",
            "Total videos for testing: 539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "633          633  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   56\n",
              "1388        1388  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   26\n",
              "274          274  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   86\n",
              "993          993  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   39\n",
              "1129        1129  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    3\n",
              "883          883  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    5\n",
              "814          814  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   52\n",
              "231          231  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   82\n",
              "691          691  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   55\n",
              "712          712  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>1388</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>993</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>814</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>231</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>691</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>712</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15d682b9-dd81-4eb3-9c6f-b1fc96505b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=4096,activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "4a010da5-cd4d-487f-97f0-0ac5b4763bc5"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "5d96710e-2e0f-4e60-d0f5-84724f35eb15"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "ed27c8ae-9a93-46ee-f747-5ebc5c79d1c1"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.6665 - accuracy: 0.5635\n",
            "Epoch 1: val_loss improved from inf to 2.04309, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 9s 491ms/step - loss: 2.6665 - accuracy: 0.5635 - val_loss: 2.0431 - val_accuracy: 0.0222\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9028\n",
            "Epoch 2: val_loss improved from 2.04309 to 0.87827, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 469ms/step - loss: 0.2349 - accuracy: 0.9028 - val_loss: 0.8783 - val_accuracy: 0.5111\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9722\n",
            "Epoch 3: val_loss did not improve from 0.87827\n",
            "16/16 [==============================] - 7s 454ms/step - loss: 0.0760 - accuracy: 0.9722 - val_loss: 1.2468 - val_accuracy: 0.4333\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9861\n",
            "Epoch 4: val_loss improved from 0.87827 to 0.77957, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 468ms/step - loss: 0.0337 - accuracy: 0.9861 - val_loss: 0.7796 - val_accuracy: 0.6111\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9980\n",
            "Epoch 5: val_loss improved from 0.77957 to 0.71336, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 7s 461ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 0.7134 - val_accuracy: 0.6667\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 0.71336\n",
            "16/16 [==============================] - 7s 455ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.9443 - val_accuracy: 0.6000\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9960\n",
            "Epoch 7: val_loss did not improve from 0.71336\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 0.0086 - accuracy: 0.9960 - val_loss: 1.2806 - val_accuracy: 0.5333\n",
            "7/7 [==============================] - 1s 141ms/step - loss: 0.3509 - accuracy: 0.9062\n",
            "Test accuracy: 90.62%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
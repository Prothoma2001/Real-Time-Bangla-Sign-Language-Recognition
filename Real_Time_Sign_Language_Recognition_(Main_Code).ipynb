{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh9nfMeqEq0Vbae2BIPQty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prothoma2001/Real-Time-Bangla-Sign-Language-Recognition-Thesis-/blob/main/Real_Time_Sign_Language_Recognition_(Main_Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid Transformer-based model"
      ],
      "metadata": {
        "id": "JW-8awm4C9XD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak8iwWqy4W0i",
        "outputId": "4c0af42f-e346-4e03-9947-2ccdbb2841a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct9883mFp03-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional\n",
        "from torch import Tensor\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2, numpy as np\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNwch3FqL2c",
        "outputId": "98fd2acf-2dc8-4a5f-f27a-830899dac0d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "label_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training')\n",
        "\n",
        "print (label_types)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBdM8BUqdnq",
        "outputId": "0636eb7d-a602-4cac-ed2f-acc173b83001"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92', '91', '96', '97', '99', '94', '98', '95', '93', '90', '89', '84', '88', '85', '82', '83', '81', '86', '87', '9', '73', '77', '78', '80', '72', '74', '75', '8', '76', '79', '7', '69', '64', '63', '68', '65', '70', '67', '66', '71', '56', '61', '58', '6', '55', '62', '54', '59', '57', '60', '46', '45', '52', '48', '50', '47', '5', '49', '51', '53', '41', '36', '42', '39', '38', '40', '43', '37', '44', '4', '33', '34', '3', '31', '29', '30', '32', '35', '28', '27', '21', '20', '19', '2', '23', '18', '24', '22', '26', '25', '14', '12', '15', '16', '11', '10', '13', '1', '0', '17']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing training data \n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Training' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n",
        "\n",
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcMlnpdq_iJ",
        "outputId": "e03d13ba-1761-4280-b232-e0ba1a5060d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tag                                         video_name\n",
            "0  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "1  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "3  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "4  92  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "     tag                                         video_name\n",
            "2087  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2088  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2089  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2090  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n",
            "2091  17  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Test Data\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('/content/drive/MyDrive/OPS22 - Main Dataset/Testing' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CdDExproGf",
        "outputId": "77bebe8b-dc9c-4f2c-bb9c-bc1c0d955610"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['91', '96', '94', '92', '93', '90', '95', '98', '97', '99', '86', '81', '82', '9', '89', '84', '85', '87', '83', '88', '73', '76', '79', '72', '77', '74', '78', '8', '75', '80', '68', '7', '69', '63', '66', '64', '67', '71', '70', '65', '59', '55', '61', '54', '62', '57', '58', '6', '60', '56', '47', '45', '49', '51', '48', '53', '5', '50', '46', '52', '38', '42', '37', '44', '43', '39', '4', '36', '40', '41', '32', '35', '29', '34', '33', '27', '31', '3', '30', '28', '18', '20', '25', '19', '2', '23', '26', '24', '21', '22', '14', '12', '16', '1', '17', '10', '13', '15', '0', '11']\n",
            "Types of activities found:  100\n",
            "  tag                                         video_name\n",
            "0  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "1  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "2  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "3  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "4  91  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "    tag                                         video_name\n",
            "737  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "738  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "739  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "740  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n",
            "741  11  /content/drive/MyDrive/OPS22 - Main Dataset/Te...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 300\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 7"
      ],
      "metadata": {
        "id": "l4fMoHS15gV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "R4X-Q2-utAYk",
        "outputId": "8271333d-0091-4b5e-812a-be8558799e1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 2092\n",
            "Total videos for testing: 742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                         video_name  tag\n",
              "1407        1407  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   44\n",
              "1010        1010  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   60\n",
              "78            78  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   97\n",
              "521          521  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   74\n",
              "1252        1252  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   41\n",
              "406          406  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    9\n",
              "1358        1358  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   40\n",
              "271          271  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   85\n",
              "630          630  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...    7\n",
              "958          958  /content/drive/MyDrive/OPS22 - Main Dataset/Tr...   54"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c830e9fe-bc41-4eb9-8ed3-c0119e877e37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>1407</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010</th>\n",
              "      <td>1010</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>78</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>521</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>1252</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>406</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>1358</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>271</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>630</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>958</td>\n",
              "      <td>/content/drive/MyDrive/OPS22 - Main Dataset/Tr...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c830e9fe-bc41-4eb9-8ed3-c0119e877e37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c830e9fe-bc41-4eb9-8ed3-c0119e877e37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c830e9fe-bc41-4eb9-8ed3-c0119e877e37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "#Change\n",
        "def build_feature_extractor(weights_path=None):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\")) #base_filter=64\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) \n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=128\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=256\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")) #base_filter=512\n",
        "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=6144,activation=\"relu\")) #base_dense=4096\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryb02Hx-57xc",
        "outputId": "13eae728-a464-49d0-8686-9406714ca875"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "DypziLE_6h4s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmPDgRp6lB7",
        "outputId": "80ff7fec-7b71-4c99-e2cc-3d081b67c394"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CUF8B6CJ6vZ7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "0AZfjP20XL29"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.2)(x)  #base_dropout=0.5\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "    plt.plot(history.history[\"accuracy\"],'r', label=\"Training Accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"],'b', label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    return history,model\n",
        "    \n",
        "trained_model = run_experiment() "
      ],
      "metadata": {
        "id": "VXeY1Qm_XhHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "1c601949-d86e-4cb8-d23b-120475876305"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5650 - accuracy: 0.5992\n",
            "Epoch 1: val_loss improved from inf to 4.24711, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 10s 528ms/step - loss: 2.5650 - accuracy: 0.5992 - val_loss: 4.2471 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9365\n",
            "Epoch 2: val_loss improved from 4.24711 to 0.43397, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 532ms/step - loss: 0.1701 - accuracy: 0.9365 - val_loss: 0.4340 - val_accuracy: 0.7778\n",
            "Epoch 3/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9861\n",
            "Epoch 3: val_loss improved from 0.43397 to 0.37259, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 8s 522ms/step - loss: 0.0428 - accuracy: 0.9861 - val_loss: 0.3726 - val_accuracy: 0.8778\n",
            "Epoch 4/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9901\n",
            "Epoch 4: val_loss did not improve from 0.37259\n",
            "16/16 [==============================] - 8s 509ms/step - loss: 0.0253 - accuracy: 0.9901 - val_loss: 2.6590 - val_accuracy: 0.4556\n",
            "Epoch 5/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9960\n",
            "Epoch 5: val_loss improved from 0.37259 to 0.36484, saving model to /tmp/video_classifier\n",
            "16/16 [==============================] - 10s 645ms/step - loss: 0.0045 - accuracy: 0.9960 - val_loss: 0.3648 - val_accuracy: 0.8778\n",
            "Epoch 6/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 0.36484\n",
            "16/16 [==============================] - 8s 510ms/step - loss: 0.0032 - accuracy: 0.9980 - val_loss: 0.8502 - val_accuracy: 0.7667\n",
            "Epoch 7/7\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 0.36484\n",
            "16/16 [==============================] - 8s 516ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4224 - val_accuracy: 0.5778\n",
            "7/7 [==============================] - 1s 136ms/step - loss: 0.3720 - accuracy: 0.9196\n",
            "Test accuracy: 91.96%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pNyF_awuVu_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence_model.save('model.h5')"
      ],
      "metadata": {
        "id": "jbTj6YIfxt52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXjkDN7uW-X6"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}